{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aefb5d67",
   "metadata": {},
   "source": [
    "### Workflow from genomes to families tables\n",
    "* Setup dir structure\n",
    "* Cluster ORFs and select reprseqs\n",
    "* Create HMM profiles with uniclust\n",
    "* Create db from set of profiles\n",
    "* Run all vs all comparison of profiles\n",
    "* Run databases search with profiles\n",
    "* Collect resulst of all-vs-all into table\n",
    "* Calculate pairwise coverage of reprseqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c821e1f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8529ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### setup ###\n",
    "\n",
    "### imports\n",
    "# python\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from subprocess import call\n",
    "\n",
    "# set paths to the virtualenv & repo\n",
    "# example:\n",
    "# pipeline_env_path = '/home/user/code/phage-pp-env'\n",
    "# lib_pp_path       = '/home/user/code/phage-pp-env/phage-pipeline'\n",
    "pipeline_env_path = '/home/user/code/phage-pp-env' # virtual environment path\n",
    "lib_pp_path       = '/home/user/code/phage-pp-env/phage-pipeline' # repo path\n",
    "sys.path.append(lib_pp_path)\n",
    "\n",
    "# import functions from setup repo\n",
    "from lib_phage.clustering import cluster_proteins\n",
    "from lib_phage.utils import setup_dir_tree, fetch_and_rename_protein_ids, build_hhr_table\n",
    "from lib_phage.utils import process_phanotate_output, create_reprseq_profile_from_clustering\n",
    "from lib_phage.utils import create_bash_script_to_parse_hhr_results, run_parsing_with_bash\n",
    "from lib_phage.utils import concatenate_parsing_results, clean_clustering_partial_data\n",
    "from lib_phage.logs import check_input_repr_prot_selection, validate_output_repr_prot_selection\n",
    "from lib_phage.logs import check_input_all_vs_all_HMM, save_params_hhblits, validate_output_hhblits\n",
    "from lib_phage.logs import validate_create_db, validate_search_all_vs_all, validate_input_ECF, validate_output_ECF\n",
    "from lib_phage.prot_compare import save_individual_seqs, run_hhblits, build_hh_db, run_all_vs_all\n",
    "from lib_phage.prot_compare import run_hhblits_dbs\n",
    "from lib_phage.ecf_finder_wrapper import load_and_filter_data, store_scan_results\n",
    "from lib_phage.repr_hits_pairwise import get_prob_cov\n",
    "\n",
    "### run mode\n",
    "# two methods of building profiles of representative sequences were tested:\n",
    "# hhblits: profiles were build by running hhblits with reprseq as a query on a UniRef database (see paper)\n",
    "# mmseqs: profiles were build from mmseqs2 results, each cluster converted to MSA\n",
    "# in the end, the hhblits was selected as final mode of building profiles for reprseq and is recommended mode here\n",
    "run_mode = 'hhblits' # profile creation mode [mmseqs/hhblits]\n",
    "\n",
    "### paths\n",
    "# work dir: select directory where all data will be stored, example:\n",
    "# work_dir = '/home/user/data/phage-pp-workdir/'\n",
    "work_dir = '/home/user/data/phage-pp-workdir/'\n",
    "setup_dir_tree(work_dir)\n",
    "\n",
    "# binaries and libraries: set paths for external software\n",
    "mmseqs_binpath  = '/home/user/mmseqs2/bin/mmseqs' # path to mmseqs2/bin/mmseqs\n",
    "uniref_db_path  = '/mnt/ramdisk/UniRef30_2020_06/UniRef30_2020_06' # path to UniRef database\n",
    "hhsuite_bins    = '/home/user/hh-suite/bin' # path to hh-suite/bin\n",
    "hhsuite_scripts = '/home/user/hh-suite/scripts' # path to hh-suite/scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b51294",
   "metadata": {},
   "source": [
    "### Setup input data:\n",
    "Copy input set of AA sequences into:<br />\n",
    "`<work_dir>/input/coding-seqs/`<br />\n",
    "They should be gathered into single multiple-sequence fasta file.<br />\n",
    "Name the file `cds-aa.fa` or change filename in the next notebook cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3e6c91",
   "metadata": {},
   "source": [
    "### Perform clustering to get representative proteins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caad14e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set clustering params\n",
    "cluster_params_min_seqid   = 0.3\n",
    "cluster_params_sensitivity = 7\n",
    "cluster_params_coverage    = 0.95\n",
    "\n",
    "# check input files integrity & if this step was already executed:\n",
    "# if it was warn about data overwrite\n",
    "if check_input_repr_prot_selection():\n",
    "\n",
    "    # perform clustering\n",
    "    clustering_filepath, clustering_msa_filepath = cluster_proteins(input_fasta_filepath=work_dir + 'input/coding-seqs/cds-aa.fa',\n",
    "                                           output_dirpath=work_dir + 'output/prot-families/representative',\n",
    "                                           mmseqs_tempdir=work_dir + 'tmp/mmseqs',\n",
    "                                           mmseqs_binpath=mmseqs_binpath,\n",
    "                                           cluster_params_min_seqid=cluster_params_min_seqid,\n",
    "                                           cluster_params_sensitivity=cluster_params_sensitivity,\n",
    "                                           cluster_params_coverage=cluster_params_coverage,\n",
    "                                           verbose=True)\n",
    "\n",
    "    no_repr_prot, name_table_filepath = fetch_and_rename_protein_ids(work_dir, clustering_filepath, \n",
    "                                                                     work_dir + 'input/coding-seqs/cds-aa.fa')\n",
    "\n",
    "    # verify output and save log file\n",
    "    validate_output_repr_prot_selection(work_dir=work_dir,\n",
    "                                        output_dirpath=work_dir + 'output/prot-families/representative',\n",
    "                                        cluster_params_min_seqid=cluster_params_min_seqid,\n",
    "                                        cluster_params_sensitivity=cluster_params_sensitivity,\n",
    "                                        cluster_params_coverage=cluster_params_coverage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f510e5",
   "metadata": {},
   "source": [
    "### Perform all vs all comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd9a2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create profiles from mmseqs clusters\n",
    "# used only for mmseqs-based profile creation\n",
    "# profile for singleton in clustering is the sequence itself\n",
    "\n",
    "if run_mode == 'mmseqs':\n",
    "    create_reprseq_profile_from_clustering(clustering_filepath, clustering_msa_filepath,\n",
    "                                           cds_all_filepath = work_dir + 'input/coding-seqs/cds-aa.fa',\n",
    "                                           profile_outdir = work_dir + 'intermediate/prot-families/profiles/mmseqs')\n",
    "else:\n",
    "    print('Running in hhblits mode, step omitted.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a3e5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create profiles for each protein with hhblits\n",
    "\n",
    "# set create profiles params\n",
    "cpu  = 16 # max number of CPUs to be used in the step\n",
    "n    = 1\n",
    "mact = 0.35\n",
    "p    = 90\n",
    "qid  = 10\n",
    "cov  = 30\n",
    "\n",
    "if run_mode == 'hhblits': # execute this only when creating profiles with hhblits\n",
    "\n",
    "    # validate previous step\n",
    "    if check_input_all_vs_all_HMM(work_dir=work_dir, force=True):\n",
    "\n",
    "        # execute current step\n",
    "        save_individual_seqs(work_dir=work_dir)\n",
    "\n",
    "        run_hhblits(work_dir=work_dir, hhsuite_bins=hhsuite_bins, hhsuite_scripts=hhsuite_scripts, cpu=cpu, \n",
    "                    uniref_db_path=uniref_db_path, n=n, mact=mact, p=p, qid=qid, cov=cov)\n",
    "\n",
    "        # save params to log\n",
    "        save_params_hhblits(work_dir=work_dir, n=n, mact=mact, p=p, qid=qid, cov=cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67884e16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Build database of profiles to be searched with hhsuite\n",
    "# You can do it two ways:\n",
    "# 1) Get set of commands that can be run in terminal (print_cmds_only=True)\n",
    "# 2) Run automatically (print_cmds_only=False)\n",
    "# First option is recommended as with large amount of data it might be useful to divide steps into smaller chunks.\n",
    "# Moreover, there was a problem with running cmds in automated way on Linux (for MacOS seemed fine).\n",
    "\n",
    "if run_mode == 'hhblits': # execute this only when creating profiles with hhblits\n",
    "\n",
    "    # validate previous step\n",
    "    if validate_output_hhblits(work_dir, run_mode=run_mode):\n",
    "\n",
    "        # execute current step\n",
    "        build_hh_db(work_dir=work_dir, hhsuite_bins=hhsuite_bins,\n",
    "                    hhsuite_scripts=hhsuite_scripts, verbose=True, run_mode=run_mode, print_cmds_only=True)\n",
    "        \n",
    "elif run_mode == 'mmseqs':\n",
    "    # execute current step\n",
    "    build_hh_db(work_dir=work_dir, hhsuite_bins=hhsuite_bins,\n",
    "                hhsuite_scripts=hhsuite_scripts, verbose=True, run_mode=run_mode, print_cmds_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1253b5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Search all vs all of reprseq profiles\n",
    "\n",
    "# set all vs all search params\n",
    "# IMPORTANT: each task takes 2 cpus fo if you have e.g. 32cpus available, set the number below to 16\n",
    "cpu  = 16 \n",
    "n    = 1\n",
    "p    = 50\n",
    "\n",
    "run_all_vs_all(work_dir=work_dir, hhsuite_bins=hhsuite_bins, \n",
    "               hhsuite_scripts=hhsuite_scripts, cpu=cpu, n=n, \n",
    "               p=p, a3m_wildcard='reprseq*a3m', run_mode=run_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f6e16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create results table\n",
    "\n",
    "# check if previous step complete\n",
    "if validate_search_all_vs_all(work_dir, run_mode=run_mode):\n",
    "    \n",
    "    # create results table\n",
    "    build_hhr_table(work_dir, run_mode=run_mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f74c975",
   "metadata": {},
   "source": [
    "### Search other databases with reprseqs (e.g. Pfam, ECOD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613b1aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Perform database search with reprseq profiles\n",
    "\n",
    "# set db path to search (hh-suite format)\n",
    "db_path  = '/mnt/ramdisk/Pfam-hh_32_0/pfam'\n",
    "db_name = 'pfam'\n",
    "\n",
    "# set database search params\n",
    "# IMPORTANT: each task takes 2 cpus fo if you have e.g. 32cpus available, set the number below to 16\n",
    "cpu  = 16\n",
    "n    = 1\n",
    "mact = 0.35\n",
    "p    = 20\n",
    "qid  = 0\n",
    "cov  = 0\n",
    "\n",
    "run_hhblits_dbs(work_dir=work_dir, hhsuite_bins=hhsuite_bins, hhsuite_scripts=hhsuite_scripts, cpu=cpu, \n",
    "            db_path=db_path, db_name=db_name, run_mode=run_mode, n=n, mact=mact, p=p, qid=qid, cov=cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63972ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Gather results from db search into table - parallel\n",
    "# parsing of result hhr files is done in parallel to speed it up\n",
    "\n",
    "# create script for parallel parsing\n",
    "create_bash_script_to_parse_hhr_results(work_dir, pipeline_env_path, lib_pp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b7f92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run parsing script\n",
    "n_cores = 32\n",
    "run_parsing_with_bash(work_dir, n_cores, db_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395f5b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if all files were parsed and gather results into one file\n",
    "if concatenate_parsing_results(work_dir, db_name):\n",
    "    clean_clustering_partial_data(work_dir, db_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cf81b8",
   "metadata": {},
   "source": [
    "### Run pairwise comparison of the coverage of reprseqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42d2718",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define paths\n",
    "output_dir = work_dir + 'output/'\n",
    "inter_dir = work_dir + 'intermediate/'\n",
    "all_by_all_output_dir = output_dir + 'prot-families/all-by-all/' + run_mode + '/'\n",
    "families_output_dir = inter_dir + 'prot-families/families/'\n",
    "reprseq_output_dir = output_dir + 'prot-families/representative/'\n",
    "parts_dir = work_dir + 'tmp/prot-families/pair_table_chunks/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e71c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load & filter table\n",
    "# load hhr_table\n",
    "table_hhr_filename = all_by_all_output_dir + 'table-hhr.txt'\n",
    "table_hhr = pd.read_csv(table_hhr_filename, sep=',')\n",
    "table_hhr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb8d983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter table by probability value\n",
    "prob_threshold = 50\n",
    "table_hhr = table_hhr[table_hhr['prob'] >= prob_threshold]\n",
    "table_hhr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97651c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get unique pairs\n",
    "# get from table_hhr qname-sname unique pairs of ids\n",
    "# drop duplicated pairs (i.e. keep only one from A->B and B->A)\n",
    "\n",
    "pair_table = table_hhr[['qname', 'sname']]\n",
    "pair_table = pair_table[pair_table['qname'] != pair_table['sname']]\n",
    "pair_table = pair_table.drop_duplicates()\n",
    "\n",
    "pair_table['names'] = pair_table.apply(lambda row: str(sorted([row.qname, row.sname])), axis=1)\n",
    "pair_table.drop_duplicates(subset=['names'], inplace=True)\n",
    "pair_table.drop(columns=['names'], inplace=True)\n",
    "\n",
    "pair_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc04342",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run comparison\n",
    "# create script to run in parallel\n",
    "# dataset will also be divided into smaller chunks to optimize computation\n",
    "n_cores = 32\n",
    "n_subsets = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0a9ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide pair table into sub-tables\n",
    "chunk_size = int(len(pair_table) / n_cores)\n",
    "pair_table_parts = []\n",
    "for i in range(n_cores-1):\n",
    "    pair_table_parts.append(pair_table[i*chunk_size:(i+1)*chunk_size])\n",
    "pair_table_parts.append(pair_table[(n_cores-1)*chunk_size:])\n",
    "\n",
    "for i, pair_table_part in enumerate(pair_table_parts):\n",
    "    pair_table_part.to_csv(parts_dir + 'pair-table-' + str(i) + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38098074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now divide into sub-subtables for smaller mem usage\n",
    "for i in range(n_cores):\n",
    "    pair_table_part = pd.read_csv(parts_dir + 'pair-table-' + str(i) + '.csv')\n",
    "    pair_table_subsets = np.array_split(pair_table_part, n_subsets)\n",
    "\n",
    "    for j, pair_sub in enumerate(pair_table_subsets):\n",
    "        pair_sub.to_csv(parts_dir + 'pair-table-' + str(i) + '-' + str(j) + '.csv', index=False)\n",
    "    os.remove(parts_dir + 'pair-table-' + str(i) + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949d2a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create subtables of results to optimize mem usages\n",
    "for i in range(n_cores):\n",
    "    for j in range(n_subsets):\n",
    "        pair_sub = pd.read_csv(parts_dir + 'pair-table-' + str(i) + '-' + str(j) + '.csv')\n",
    "        qname_list = list(pair_sub['qname'].unique())\n",
    "        sname_list = list(pair_sub['sname'].unique())\n",
    "        \n",
    "        df1 = table_hhr[(table_hhr['qname'].isin(qname_list)) & (table_hhr['sname'].isin(sname_list))]\n",
    "        df2 = table_hhr[(table_hhr['qname'].isin(sname_list)) & (table_hhr['sname'].isin(qname_list))]\n",
    "        df = pd.concat([df1, df2])\n",
    "        df.drop_duplicates(inplace=True)\n",
    "        \n",
    "        df.to_csv(parts_dir + 'table-hhr-' + str(i) + '-' + str(j) + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ea80ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallel python script to process sub-table\n",
    "run_cores = [0,32] # subset to run\n",
    "\n",
    "script_filepath =  work_dir + 'tmp/prot-families/run-pairwise-hits.sh'\n",
    "ecf=False\n",
    "\n",
    "cmd = '#!/bin/bash\\n\\n'\n",
    "cmd += 'source ' + pipeline_env_path + '/bin/activate\\n'\n",
    "\n",
    "for i in range(run_cores[0], run_cores[1]):\n",
    "    pair_table_path = parts_dir + 'pair-table-' + str(i) + '.csv'    \n",
    "    cmd += 'nohup python3 {}/lib_phage/run_hits_pairwise_single_table.py {} {} {} {} {} {} {} &\\n'.format(\n",
    "    pipeline_env_path, work_dir, run_mode, i, prob_threshold, n_subsets, ecf, lib_pp_path)\n",
    "\n",
    "with open(script_filepath, 'w') as file_sh:\n",
    "    file_sh.write(cmd)\n",
    "    \n",
    "print('RUN MANUALLY script from ' + work_dir + 'tmp/prot-families')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bc75c7",
   "metadata": {},
   "source": [
    "#### Make sure you run script as instructed by previous cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3c6d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate results, add header\n",
    "part_results_filepath = families_output_dir + 'repr-hits-pairwise-prob' + str(prob_threshold) + '-*.txt'\n",
    "concat_results_filepath = output_dir + 'prot-families/families/repr-hits-pairwise-prob' + str(prob_threshold) + '.txt'\n",
    "header_path = output_dir + 'prot-families/families/header'\n",
    "\n",
    "call('echo \"qname,sname,prob,scov_min,scov_max,qcov_min,qcov_max,pident,max_cov,min_cov\" > ' + header_path, shell=True)\n",
    "call('cat ' + part_results_filepath + ' > ' + concat_results_filepath, shell=True)\n",
    "call('cat ' + header_path + ' ' + concat_results_filepath + ' > ' + concat_results_filepath.replace('.txt', '.csv'),\n",
    "     shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7f1d26",
   "metadata": {},
   "source": [
    "### Prepare file for MCL clustering of reprseqs by pairwise comparison (group into protein families)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c756aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create MCL input\n",
    "pair_hits_filename = 'repr-hits-pairwise-prob' + str(prob_threshold) + '.csv'\n",
    "pair_hits_filepath = output_dir + 'prot-families/families/' + pair_hits_filename\n",
    "\n",
    "pair_table_full = pd.read_csv(pair_hits_filepath)\n",
    "pair_table_full['qcov'] = pair_table_full.apply(lambda x: max(x.qcov_min, x.qcov_max), axis=1)\n",
    "pair_table_full['scov'] = pair_table_full.apply(lambda x: max(x.scov_min, x.scov_max), axis=1)\n",
    "pair_table_full['fam_cov'] = pair_table_full.apply(lambda x: min(x.qcov, x.scov), axis=1)\n",
    "pair_table_full['weight'] = pair_table_full.apply(lambda x: x.prob * min(x.qcov, x.scov), axis=1)\n",
    "\n",
    "pair_table_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee9b715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter table by qcov & scov values\n",
    "family_cov = 0.8\n",
    "family_prob = 0.95\n",
    "pair_table_full = pair_table_full[pair_table_full['prob'] >= family_prob]\n",
    "pair_table_full = pair_table_full[pair_table_full['fam_cov'] >= family_cov]\n",
    "pair_table_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd00307e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create MCL input table\n",
    "pair_table_mcl = pair_table_full[['qname', 'sname', 'weight']]\n",
    "pair_table_mcl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b833b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add singletons, i.e. all reprseq that does not have any hit and therefore are out of the table\n",
    "# encode singletons as: reprseq_id, reprseq_id, 1\n",
    "reprseq_nametable = pd.read_csv(reprseq_output_dir + 'name-table.txt')\n",
    "qset = set(pair_table_mcl['qname'].unique())\n",
    "sset = set(pair_table_mcl['sname'].unique())\n",
    "reprseq_present = qset.union(sset)\n",
    "\n",
    "rseq_names = set(reprseq_nametable['repr.name'].unique())\n",
    "rseq_missing = rseq_names.difference(reprseq_present)\n",
    "len(rseq_missing)\n",
    "\n",
    "rseq_singletons = {i:[rseq_name, rseq_name, 1.0] for i, rseq_name in enumerate(list(rseq_missing))}\n",
    "rseq_singletons_table = pd.DataFrame.from_dict(rseq_singletons, orient='index', columns=['qname', 'sname', 'weight'])\n",
    "rseq_singletons_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c3fe3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate filtered table with singleton table\n",
    "pair_table_mcl_all = pd.concat([pair_table_mcl, rseq_singletons_table])\n",
    "pair_table_mcl_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dabcb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop final input file for MCL\n",
    "# perform MCL outside of this notebook\n",
    "pair_table_mcl_all.to_csv(pair_hits_filepath.replace('.csv', '-cov' + str(int(family_cov*100)) + '-mcl-in.abc'), sep=' ', index=False, header=False, float_format=\"%.4f\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
